 Deep Learning - Back Propagation

- 9장 - 오차 역전파에서 딥러닝으로!
    
    ### Intro
    
    - XOR 문제를 다층 퍼셉트론으로 해결! But, 은닉층에 포함된 가중치를 업데이트할 방법 x
    - 인공지능 겨울☃️ → 제프리 힌튼 교수의 오차 역전파 → 딥러닝
    
    ### 오차 역전파에 대해
    
    - Back-Propagation 한 줄 설명
        - 컴퓨터는 주어진 입력값을 신경망을 거쳐 출력값으로 반환
        - 신경망은 <입력 - 신경망 - 출력>과 같이 좌측에서 우측으로의 진행방향을 가지지만, 훈련을 위해서 오차 역전파(Back-propagation) 과정에서는 이와는 반대의 진행방향을 가짐.
        - 오차 역전파 과정은 컴퓨터가 예측값의 정확도를 높이기 위해 출력값과 실제 예측하고자 하는 값을 비교하여 가중치를 변경하는 작업을 말한다!
        - 즉, 알고리즘은 신경망의 가중치를 조정하기 위해 출력값과 정답 사이의 오차를 이용하여 역방향으로 각 층의 가중치를 업데이트하는 방법.
            - 역전파 단계에서는 오차를 각 층의 가중치로 역방향으로 전달하며, 이를 통해 가중치의 조정이 이뤄짐.
    
    ---
    
    - 다층 퍼셉트론을 활용한 XOR 문제 해결 시 : input과 output 알고 있는 상태에서 가중치와 바이어스를 미리 알아본 후 이를 집어넣음
    - But 우리는 데이터를 통해 스스로 가중치를 조절하는 학습이 필요
    - 경사 하강법 (임의의 가중치 선언 → 결괏값을 이용해 오차 구하기 → 이 오차가 최소인 지점으로 계속해서 조금씩 이동시키는 방법)
        - 오차가 최소인 지즘 = 미분했을 때 기울기 = 0
        - But, 경사하강법은 단일 퍼셉트론만 가능
        - 은닉층이 생기면, 두 번의 경사 하강법 필요
            - 1. 출력층 가중치 수정
            - 2. 은닉층 가중치 수정
            - 상세 설명
                1. 주어진 입력값에 상관없이, 임의의 초기 가중치(w)를 준 뒤 은닉층을 거쳐 결과를 계산
                2. 계산 결과와 실제 예측하고자 하는 값 사이의 오차를 구함
                3. 가중치 업데이트
                4. '1~3'의 과정을 오차가 더이상 줄어들지 않을 때까지 반복 
    - 오차 역전파 공식
        
        공식
        
        - t+1 = 현재 단계의 계산
        - 오차 Y_out을 구한 후 이를 W에 대해 편미분
        - 오차를 구할 땐 평균 제곱 오차를 사용!
            
            공식
            
        - 이와 같이 W를 구할 땐 오차 공식을 구하고 W에 대해 편미분 후 업데이트
        - 은닉층의 가중치 업데이트
        
        공식
        
        - 델타식은 out(1-out)
        - 은닉층의 숫자가 늘어도 이 형태가 계속해서 나타나므로, 깊은 층을 통해 학습하는 딥러닝도 가능해짐
        - 참고 : [https://knowledgeforengineers.tistory.com/123](https://knowledgeforengineers.tistory.com/123)
    
    ###
